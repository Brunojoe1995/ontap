---
permalink: system-admin/remov-nodes-cluster-concept.html
sidebar: sidebar
keywords: remove, node, cluster, failover, partner, erase, inaccessible
summary: "You can remove unwanted nodes from a cluster, one node at a time. After you remove a node, you must also remove its failover partner. If you are removing a node, then its data becomes inaccessible or erased."
---
= Remove nodes from the cluster
:icons: font
:imagesdir: ../media/

[.lead]
You can remove unwanted nodes from a cluster, one node at a time. After you remove a node, you must also remove its failover partner. If you are removing a node, then its data becomes inaccessible or erased.

.Before you begin

The following conditions must be satisfied before removing nodes from the cluster:

* More than half of the nodes in the cluster must be healthy.
* All of the data on the node that you want to remove must have been evacuated.
** This might include link:../encryption-at-rest/secure-purge-data-encrypted-volume-concept.html[purging data from an encrypted volume].

* All volumes have been link:../volumes/move-volume-task.html[moved] or link:../volumes/delete-flexvol-task.html[deleted] from aggregates owned by the node.

* All aggregates have been link:../disks-aggregates/commands-manage-aggregates-reference.html[deleted] from the node.

* If the node owns self-encrypting disks (SEDs), link:../encryption-at-rest/return-seds-unprotected-mode-task.html[disk encryption has been removed] by returning SEDs.
** You might also want to link:../encryption-at-rest/sanitize-fips-drive-sed-task.html[sanitize FIPS or SED drives].

* Data LIFs have been link:../networking/delete_a_lif.html[deleted] or link:../networking/migrate_a_lif.html[relocated] from the node.

* Cluster management LIFs have been link:../networking/migrate_a_lif.html[relocated] from the node and the home ports changed.

* All intercluster LIFs have been link:../networking/delete_a_lif.html[removed].
** When you remove intercluster LIFs a warning is displayed that can be ignored.

* Storage failover has been link:../high-availability/ha_commands_for_enabling_and_disabling_storage_failover.html[disabled] for the node.

* All LIF failover rules have been link:../networking/commands_for_managing_failover_groups_and_policies.html[modified] to remove ports on the node.

* All VLANs on the node have been link:../networking/configure_vlans_over_physical_ports.html#delete-a-vlan[deleted].

It is recommended that you issue an AutoSupport message to notify NetApp technical support that node removal is underway.

*Note:* You must not perform operations such as `cluster remove-node`, `cluster unjoin`, and `node rename` when an automated ONTAP upgrade is in progress.

.About this task

If you are running a mixed-version cluster, you can remove the last low-version node by using one of the advanced privilege commands beginning with ONTAP 9.3:

* ONTAP 9.3: `cluster unjoin -skip-last-low-version-node-check`
* ONTAP 9.4 and later: `cluster remove-node -skip-last-low-version-node-check`

*Note:* You must make all of the system and user data from the disks that are connected to the node inaccessible to users before removing a node from the cluster. After removing a node from the cluster, if you need to join the node back to the same cluster, contact technical support for assistance about restoring data.

.Steps

. Change the privilege level to advanced:
+
`*set -privilege advanced*`

. If the node you want to remove is the current master node, then enable another node in the cluster to be elected as the master node by changing the master node's cluster eligibility to `false`:
+
`*cluster modify â€“eligibility false*`
+
The master node is the node that holds processes such as "`mgmt`", "`vldb`", "`vifmgr`", "`bcomd`", and "`crs`". The `cluster ring show` advanced command shows the current master node.
+
----
cluster::*> cluster modify -node node1 -eligibility false
----

. Remove the node from the cluster:
+
[options="header"]
|===
| For this ONTAP version...| Use this command...
a|
ONTAP 9.3
a|
`*cluster unjoin*`
a|
ONTAP 9.4 and later
a|
`*cluster remove-node*`
|===
If you have a mixed version cluster and you are removing the last lower version node, use the `-skip-last-low-version-node-check` parameter with these commands.
+
The system informs you of the following:

 ** You must also remove the node's failover partner from the cluster.
 ** After the node is removed and before it can rejoin a cluster, you must use boot menu option (4) Clean configuration and initialize all disks or option (9) Configure Advanced Drive Partitioning to erase the node's configuration and initialize all disks.
+
A failure message is generated if you have conditions that you must address before removing the node. For example, the message might indicate that the node has shared resources that you must remove or that the node is in a cluster HA configuration or storage failover configuration that you must disable.
+
If the node is the quorum master, the cluster will briefly lose and then return to quorum. This quorum loss is temporary and does not affect any data operations.

. If a failure message indicates error conditions, address those conditions and rerun the `cluster remove-node` or `cluster unjoin` command.
+
The node is automatically rebooted after it is successfully removed from the cluster.

. If you are repurposing the node, erase the node configuration and initialize all disks:
 .. During the boot process, press Ctrl-C to display the boot menu when prompted to do so.
 .. Select the boot menu option *(4) Clean configuration and initialize all disks*.
. Return to admin privilege level:
+
`*set -privilege admin*`
. Repeat the preceding steps to remove the failover partner from the cluster.

.After you finish

If you removed nodes to have a single-node cluster, you should modify the cluster ports to serve data traffic by modifying the cluster ports to be data ports, and then creating data LIFs on the data ports.

// 2022-03-10, BURT 1453521
